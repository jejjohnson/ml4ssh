{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69d67b99-71be-4f66-9397-b15b60ee0226",
   "metadata": {},
   "source": [
    "# NerF + QG Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59583528-46e3-4e59-99c6-338029736d60",
   "metadata": {},
   "source": [
    "The full QG equation is given by:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\partial_t q + \\det \\boldsymbol{J}(q, \\psi) &= 0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $q=\\nabla^2 \\psi$\n",
    "* $\\det \\boldsymbol{J}(q, \\psi)=\\partial_x q\\partial_y\\psi - \\partial_y q\\partial_x\\psi$.\n",
    "\n",
    "We are interested in finding some NerF method that can take in the spatial-temporal coordinates, $\\mathbf{x}_\\phi$, and output a vector corresponding to the PV and stream function, $\\psi$, i.e. $\\mathbf{y}_\\text{obs}$.\n",
    "\n",
    "$$\n",
    "\\mathbf{y}_\\text{obs} = \\boldsymbol{f_\\theta}(\\mathbf{x}_\\phi) + \\epsilon, \\hspace{5mm}\\epsilon \\sim \\mathcal{N}(0, \\sigma^2)\n",
    "$$\n",
    "\n",
    "We use a SIREN network which is a fully connected neural network with the $sin$ activation function.\n",
    "\n",
    "* **Data Inputs**: `256x256x11`\n",
    "* **Data Ouputs**: `2`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d850f86-a38d-41f6-972c-4ecb70a3af42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from pyprojroot import here\n",
    "\n",
    "# spyder up to find the root\n",
    "root = here(project_files=[\".root\"])\n",
    "\n",
    "# append to path\n",
    "sys.path.append(str(root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbd0d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from ml_collections import config_dict\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "\n",
    "pl.seed_everything(123)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import wandb\n",
    "\n",
    "sns.reset_defaults()\n",
    "sns.set_context(context=\"talk\", font_scale=0.7)\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9788b8f2-9941-4da8-b906-038115cb8ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_logger = WandbLogger(\n",
    "    config=args,\n",
    "    mode=log_options.mode,\n",
    "    project=log_options.project,\n",
    "    entity=log_options.entity,\n",
    "    dir=log_options.log_dir,\n",
    "    resume=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e3f437",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /Users/eman/code_projects/torchqg/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce7bc90-c211-4b4a-be9a-366b7be440c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = f\"/Users/eman/code_projects/torchqg/data/qgsim_simple_128x128.zarr\"\n",
    "save_dir = f\"/Users/eman/code_projects/torchqg/data/qgsim_simple_128x128.nc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8100fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = xr.open_dataset(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b90747",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386f5629-aa35-4ff9-a8b1-72d6b99b25b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def reformat_time(ds, dt):\n",
    "\n",
    "#     ds = ds - ds[0]\n",
    "#     ds /= dt\n",
    "\n",
    "#     return ds\n",
    "\n",
    "\n",
    "# def preprocessing(\n",
    "#     ds, noise: float=0.01, dt: float=1.0,\n",
    "#     time_min: int=500,\n",
    "#     time_max: int=511,\n",
    "#     seed: int=123\n",
    "# ):\n",
    "\n",
    "#     # slice timesteps\n",
    "#     ds = ds.isel(steps=slice(time_min,time_max))\n",
    "\n",
    "#     # reformat time\n",
    "#     time_coords = ds.steps.values.astype(np.float64)\n",
    "\n",
    "#     time_coords = reformat_time(\n",
    "#         time_coords, dt\n",
    "#     )\n",
    "#     time_coords = time_coords.astype(np.float64)\n",
    "#     ds[\"steps\"] = time_coords\n",
    "\n",
    "#     # add noise to observations\n",
    "#     rng = np.random.RandomState(seed)\n",
    "\n",
    "#     ds[\"q_obs\"] = ds[\"q\"] + noise * rng.randn(*ds[\"q\"].shape)\n",
    "#     ds[\"p_obs\"] = ds[\"p\"] + noise * rng.randn(*ds[\"p\"].shape)\n",
    "\n",
    "#     return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d4de24-69d8-47fd-bbe8-1ae75e288ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = preprocessing(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39320a1e-9a01-425c-9142-154e439d5248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.to_netcdf(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a786de-6f95-4b65-8582-d00b30a05875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.p.thin(steps=1).plot.imshow(\n",
    "#     col=\"steps\",\n",
    "#     robust=True,\n",
    "#     col_wrap=3,\n",
    "#     cmap=\"viridis\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b8a482-4055-45b1-b37d-77ebfa6dafbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.p.hvplot.image(x=\"Nx\", y=\"Ny\", width=500, height=400, cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e3fbbf-49a0-4081-bd6c-6d33db53fb6a",
   "metadata": {},
   "source": [
    "Now, we need to transform the dataset such we get all of the coordinates. Currently the `(x,y,t)` coordinates are vectors and the datasets, `(p,q,u,v)` are multidimensional arrays in terms of the vectors `(x,y,t)`. So we need to replicate each coordinate vector such that we get a grid, i.e. `x->(x,y,t)`.\n",
    "\n",
    "**Note**: Typically, we need to do meshgrid but we can do this automatically with the xarray `to_dataframe().reset_index()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37716637-b2ae-4ab8-aef5-d25f0eaa0eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create indices\n",
    "data_df = data.to_dataframe().reset_index()\n",
    "\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c909c4-518f-4a51-b18f-0aeb0780b10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset variables of interest\n",
    "x_df = data_df[[\"Nx\", \"Ny\", \"steps\"]]\n",
    "y_df = data_df[[\"p\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7add9a96-5070-4857-bd6c-b3aebfafe98d",
   "metadata": {},
   "source": [
    "### Rescale\n",
    "\n",
    "We will rescale the spatial and temporal dimensions to be between -1 and 1. This is necessary for the SIREN network.\n",
    "\n",
    "We use the following formula for the forward transform:\n",
    "\n",
    "$$\n",
    "T(x) = \\frac{x - a}{b-a}\n",
    "$$\n",
    "\n",
    "and the following formula for the inverse transformation\n",
    "\n",
    "$$\n",
    "T^{-1}(x) = x (b - a) + a\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4829ae41-1c99-45e8-9f5e-a9dfdf18901e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputScalingTransform(nn.Module):\n",
    "    def __init__(self, x_min, x_max):\n",
    "        super().__init__()\n",
    "\n",
    "        self.register_buffer(\"x_min\", torch.FloatTensor(x_min))\n",
    "        self.register_buffer(\"x_max\", torch.FloatTensor(x_max))\n",
    "\n",
    "    def forward(self, x, inverse=False):\n",
    "        if not inverse:\n",
    "            return self.transform(x)\n",
    "        else:\n",
    "            return self.inverse_transform(x)\n",
    "\n",
    "    def transform(self, x):\n",
    "        return (x - self.x_min) / (self.x_max - self.x_min)\n",
    "\n",
    "    def inverse_transform(self, x):\n",
    "        return x * (self.x_max - self.x_min) + self.x_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56da43df-5500-4d1e-ac23-132a46680343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get min/max values of dataset\n",
    "x_min = x_df.min(axis=0)\n",
    "x_max = x_df.max(axis=0)\n",
    "\n",
    "x_min, x_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2913f85f-86a5-45f1-bd25-c5329469c203",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = InputScalingTransform(x_min.values, x_max.values)\n",
    "\n",
    "x_torch = torch.FloatTensor(x_df.values)\n",
    "x_torch_t = transform(x_torch)\n",
    "\n",
    "# check shapes\n",
    "assert x_torch_t.shape == x_torch.shape\n",
    "\n",
    "# check min/max values\n",
    "assert x_torch_t.min().item() == 0.0\n",
    "assert x_torch_t.max().item() == 1.0\n",
    "\n",
    "# check min/max value locations\n",
    "np.testing.assert_array_equal(\n",
    "    x_torch_t.min(dim=0)[1].numpy(), x_torch.min(dim=0)[1].numpy()\n",
    ")\n",
    "np.testing.assert_array_equal(\n",
    "    x_torch_t.max(dim=0)[1].numpy(), x_torch.max(dim=0)[1].numpy()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361a98e8-5be6-4df5-add3-b0b87e620ad8",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d48702c-2739-43f2-afaf-e237e3d24441",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27641e7-fd8f-407f-833e-0f34a20873fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_ds = TensorDataset(\n",
    "    torch.FloatTensor(x_df.values), torch.FloatTensor(y_df.values)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360f3bea-b4cd-4a92-a31e-30e351108a1e",
   "metadata": {},
   "source": [
    "### Train Test Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047db56d-7c7f-49ae-a906-a4276365cbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_datapoints = len(predict_ds)\n",
    "train_split = int(0.9 * n_datapoints)\n",
    "valid_split = n_datapoints - train_split\n",
    "\n",
    "# random split\n",
    "train_ds, valid_ds = torch.utils.data.random_split(\n",
    "    predict_ds, (train_split, valid_split)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512c224c-b0eb-4cd6-8363-d0d3bef012ff",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b31830-7741-446a-9fcb-bb04cb3bb902",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1096\n",
    "num_workers = 0\n",
    "pin_memory = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a3aa46-faa1-4d47-b007-1a2484a108e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    ")\n",
    "\n",
    "valid_dl = DataLoader(\n",
    "    valid_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    ")\n",
    "\n",
    "test_dl = DataLoader(\n",
    "    predict_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    ")\n",
    "\n",
    "predict_dl = DataLoader(\n",
    "    predict_ds,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4858b20-4c08-496b-aa14-64d72cc6b65f",
   "metadata": {},
   "source": [
    "## Data Module\n",
    "\n",
    "Now we will put all of the preprocessing routines together. This is **very important** for a few reasons:\n",
    "\n",
    "1. It collapses all of the operations in a modular way\n",
    "2. It makes it reproducible for the next people\n",
    "3. It makes it very easy for the PyTorch-Lightning framework down the line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba5b2cc-3ed6-4a3f-a3f6-abfe9ded4592",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_collections import config_dict\n",
    "\n",
    "cfg = config_dict.ConfigDict()\n",
    "\n",
    "# data args\n",
    "cfg.data = config_dict.ConfigDict()\n",
    "cfg.data.data_dir = f\"/Users/eman/code_projects/torchqg/data/qgsim_simple_128x128.nc\"\n",
    "\n",
    "# preprocessing args\n",
    "cfg.pre = config_dict.ConfigDict()\n",
    "cfg.pre.noise = 0.01\n",
    "cfg.pre.dt = 1.0\n",
    "cfg.pre.time_min = 500\n",
    "cfg.pre.time_max = 511\n",
    "cfg.pre.seed = 123\n",
    "\n",
    "# train/test args\n",
    "cfg.split = config_dict.ConfigDict()\n",
    "cfg.split.train_prct = 0.9\n",
    "\n",
    "# dataloader args\n",
    "cfg.dl = config_dict.ConfigDict()\n",
    "cfg.dl.batchsize_train = 512\n",
    "cfg.dl.batchsize_val = 1_000\n",
    "cfg.dl.batchsize_test = 2_000\n",
    "cfg.dl.batchsize_predict = 2_000\n",
    "cfg.dl.num_workers = 0\n",
    "cfg.dl.pin_memory = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da06ade-90f7-41ad-9165-6c05a876f05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45e0af1-1243-48d1-b593-80be9cde3491",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputScalingTransform(nn.Module):\n",
    "    def __init__(self, x_min, x_max):\n",
    "        super().__init__()\n",
    "\n",
    "        self.register_buffer(\"x_min\", torch.FloatTensor(x_min))\n",
    "        self.register_buffer(\"x_max\", torch.FloatTensor(x_max))\n",
    "\n",
    "    def forward(self, x, inverse=False):\n",
    "        if not inverse:\n",
    "            return self.transform(x)\n",
    "        else:\n",
    "            return self.inverse_transform(x)\n",
    "\n",
    "    def transform(self, x):\n",
    "        return (x - self.x_min) / (self.x_max - self.x_min)\n",
    "\n",
    "    def inverse_transform(self, x):\n",
    "        return x * (self.x_max - self.x_min) + self.x_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75257127-cb80-43d1-9d5a-16e0fa50a32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QGSimulation(pl.LightningModule):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "\n",
    "        # load data\n",
    "        data = xr.open_dataset(self.config.data.data_dir, engine=\"netcdf4\")\n",
    "\n",
    "        data_df = data.to_dataframe().reset_index()\n",
    "\n",
    "        # subset variables of interest\n",
    "        x_df = data_df[[\"Nx\", \"Ny\", \"steps\"]]\n",
    "        y_df = data_df[[\"p\"]]\n",
    "\n",
    "        # get spatial/temporal min/max limits\n",
    "        x_min = x_df.min(axis=0)\n",
    "        x_max = x_df.max(axis=0)\n",
    "\n",
    "        # create invertible transformation\n",
    "        transform = InputScalingTransform(x_min.values, x_max.values)\n",
    "        self.transform = transform\n",
    "\n",
    "        # create prediction dataset (everything)\n",
    "        predict_ds = TensorDataset(\n",
    "            torch.FloatTensor(x_df.values), torch.FloatTensor(y_df.values)\n",
    "        )\n",
    "\n",
    "        # create train/val/test datasets\n",
    "        n_datapoints = len(predict_ds)\n",
    "        train_split = int(self.config.split.train_prct * n_datapoints)\n",
    "        valid_split = n_datapoints - train_split\n",
    "\n",
    "        # random split\n",
    "        train_ds, valid_ds = torch.utils.data.random_split(\n",
    "            predict_ds, (train_split, valid_split)\n",
    "        )\n",
    "\n",
    "        self.ds_train = train_ds\n",
    "        self.ds_valid = valid_ds\n",
    "        self.ds_predict = predict_ds\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.ds_train,\n",
    "            batch_size=self.config.dl.batchsize_train,\n",
    "            shuffle=True,\n",
    "            num_workers=self.config.dl.num_workers,\n",
    "            pin_memory=self.config.dl.pin_memory,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.ds_valid,\n",
    "            batch_size=self.config.dl.batchsize_val,\n",
    "            shuffle=False,\n",
    "            num_workers=self.config.dl.num_workers,\n",
    "            pin_memory=self.config.dl.pin_memory,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.ds_predict,\n",
    "            batch_size=self.config.dl.batchsize_test,\n",
    "            shuffle=False,\n",
    "            num_workers=self.config.dl.num_workers,\n",
    "            pin_memory=self.config.dl.pin_memory,\n",
    "        )\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.ds_predict,\n",
    "            batch_size=self.config.dl.batchsize_predict,\n",
    "            shuffle=False,\n",
    "            num_workers=self.config.dl.num_workers,\n",
    "            pin_memory=self.config.dl.pin_memory,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67da7217-f0a7-42b9-9351-db849446c975",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = QGSimulation(cfg)\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f924f943-f3bd-481c-9e20-e1fc9403aaa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_init, y_init = dm.ds_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b14ad86-9688-4d02-8fa8-5e691763a3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_init.shape, y_init.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a68f60-19d7-439e-988b-8a99263ee269",
   "metadata": {},
   "source": [
    "## NerF\n",
    "\n",
    "This standard Neural Fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3a5668-1ac3-440c-bc8c-5fbbc485ea69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inr4ssh._src.models.siren import Siren, SirenNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2f1b7f-e81c-4f96-a310-f766872f0368",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_in = x_init.shape[1]\n",
    "dim_hidden = 256\n",
    "dim_out = y_init.shape[1]\n",
    "num_layers = 4\n",
    "w0 = 1.0\n",
    "w0_initial = 30.0\n",
    "c = 6.0\n",
    "final_activation = None  # nn.Sigmoid()\n",
    "\n",
    "net = SirenNet(\n",
    "    dim_in=dim_in,\n",
    "    dim_hidden=dim_hidden,\n",
    "    dim_out=dim_out,\n",
    "    num_layers=num_layers,\n",
    "    w0=w0,\n",
    "    w0_initial=w0_initial,\n",
    "    c=c,\n",
    "    final_activation=final_activation,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b8b3c9-60a0-4cea-89f9-8f16daf2b66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = net(x_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54a5b16-265f-4aa9-9ac8-7316fcbdfa8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d98a0ab8-3e63-44f1-98d7-3a5541e2ab9c",
   "metadata": {},
   "source": [
    "## PINNS Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4409bd4-cbce-4308-9425-587a2382db84",
   "metadata": {},
   "source": [
    "$$\n",
    "\\partial_t \\nabla^2 \\psi + \\det J(\\psi, \\nabla^2\\psi) = 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d099c57-9f81-445b-8870-5429d1caa3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inr4ssh._src.operators import differential_simp as diffops_simp\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bcc820-3187-4579-87fb-cd5c0daef320",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegQG(nn.Module):\n",
    "    def __init__(self, alpha: float = 1e-4):\n",
    "        super().__init__()\n",
    "\n",
    "        alpha = torch.Tensor([alpha])\n",
    "\n",
    "        self.register_buffer(\"alpha\", alpha)\n",
    "\n",
    "    def forward(self, x, f):\n",
    "\n",
    "        with torch.set_grad_enabled(True):\n",
    "\n",
    "            x = torch.autograd.Variable(x, requires_grad=True)\n",
    "\n",
    "            u = f(x)\n",
    "\n",
    "            grad_nn = diffops_simp.gradient(u, x)\n",
    "            q_nn = diffops_simp.divergence(grad_nn, x)\n",
    "            dlaplacU = diffops_simp.gradient(q_nn, x)\n",
    "            Jacob_U_laplacU = (\n",
    "                grad_nn[:, 1] * dlaplacU[:, 2] - grad_nn[:, 2] * dlaplacU[:, 1]\n",
    "            )\n",
    "\n",
    "            pde_loss = F.mse_loss(\n",
    "                dlaplacU[:, 0] + Jacob_U_laplacU, torch.zeros_like(Jacob_U_laplacU)\n",
    "            )\n",
    "            return self.alpha * pde_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5d619e-b6f9-4e11-88e3-eec1665eb919",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_loss = RegQG(1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6214a0-7a2c-4a4a-9930-61756970a838",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_loss(x_init, net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea7ab29-f515-465d-a4c5-1f786806008e",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d5ee8c-a204-48dc-a47f-fbc66d88646d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pl_bolts.optimizers.lr_scheduler import LinearWarmupCosineAnnealingLR\n",
    "from functools import partial\n",
    "from typing import Dict, Any, cast\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35508e0c-bdf9-4909-b898-208a2617daf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class INRModel(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        reg_pde,\n",
    "        optimizer: str = \"adam\",\n",
    "        qg: bool = True,\n",
    "        alpha: float = 0.1,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        self.model = model\n",
    "        self.hyperparams = cast(Dict[str, Any], self.hparams)\n",
    "        self.loss_data = nn.MSELoss(reduction=\"mean\")\n",
    "        self.reg_pde = RegQG(self.hyperparams.get(\"alpha\", 1e-4))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def _data_loss(self, batch):\n",
    "        x, y = batch\n",
    "\n",
    "        pred = self.forward(x)\n",
    "\n",
    "        # parse inputs\n",
    "        x, y = batch\n",
    "\n",
    "        # data loss function\n",
    "        loss = self.loss_data(y, pred)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def _qg_loss(self, batch):\n",
    "\n",
    "        x, y = batch\n",
    "\n",
    "        loss = self.reg_pde.forward(x, self.model)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        # loss function\n",
    "        loss_data = self._data_loss(batch)\n",
    "\n",
    "        if self.hyperparams.get(\"qg\", False):\n",
    "            # x_var = torch.autograd.Variable(x, requires_grad=True)\n",
    "            # out = self.forward(x_var)\n",
    "            # reg = qg_loss(out, x_var, 1.0, 1.0, 1.0, \"mean\")\n",
    "            loss_reg = self._qg_loss(batch)\n",
    "\n",
    "            loss = loss_data + loss_reg\n",
    "\n",
    "            self.log(\"train_reg\", loss_reg, prog_bar=True)\n",
    "            self.log(\"train_data\", loss_data, prog_bar=True)\n",
    "        else:\n",
    "            loss = loss_data\n",
    "\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "\n",
    "        # loss function\n",
    "        loss_data = self._data_loss(batch)\n",
    "\n",
    "        if self.hyperparams.get(\"qg\", False):\n",
    "            # x_var = torch.autograd.Variable(x, requires_grad=True)\n",
    "            # out = self.forward(x_var)\n",
    "            # reg = qg_loss(out, x_var, 1.0, 1.0, 1.0, \"mean\")\n",
    "            loss_reg = self._qg_loss(batch)\n",
    "\n",
    "            loss = loss_data + loss_reg\n",
    "\n",
    "            self.log(\"train_reg\", loss_reg, prog_bar=True)\n",
    "            self.log(\"train_data\", loss_data, prog_bar=True)\n",
    "        else:\n",
    "            loss = loss_data\n",
    "\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "\n",
    "        # loss function\n",
    "        loss_data = self._data_loss(batch)\n",
    "\n",
    "        if self.hyperparams.get(\"qg\", False):\n",
    "            # x_var = torch.autograd.Variable(x, requires_grad=True)\n",
    "            # out = self.forward(x_var)\n",
    "            # reg = qg_loss(out, x_var, 1.0, 1.0, 1.0, \"mean\")\n",
    "            loss_reg = self._qg_loss(batch)\n",
    "\n",
    "            loss = loss_data + loss_reg\n",
    "\n",
    "            self.log(\"train_reg\", loss_reg, prog_bar=True)\n",
    "            self.log(\"train_data\", loss_data, prog_bar=True)\n",
    "        else:\n",
    "            loss = loss_data\n",
    "\n",
    "        self.log(\"test_loss\", loss, prog_bar=True)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        # output\n",
    "        x, y = batch\n",
    "\n",
    "        pred = self.forward(x)\n",
    "\n",
    "        return pred\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        # configure optimizer\n",
    "        optimizer = Adam(self.model.parameters(), lr=self.hyperparams.get(\"lr\", 1e-4))\n",
    "\n",
    "        scheduler = LinearWarmupCosineAnnealingLR(\n",
    "            optimizer,\n",
    "            warmup_epochs=self.hyperparams.get(\"warmup\", 10),\n",
    "            max_epochs=self.hyperparams.get(\"num_epochs\", 100),\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": scheduler,\n",
    "            \"monitor\": \"val_loss\",\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d614a086-9047-409a-bca0-a6c5f5397560",
   "metadata": {},
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8eb6f69-6ac9-4c63-92c3-5cba761c4005",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import TQDMProgressBar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad49aef1-e25b-439e-b7ba-32bfa9156080",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [TQDMProgressBar(refresh_rate=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac691f0e-8563-466b-ba03-aa5e036a6937",
   "metadata": {},
   "source": [
    "### Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749179c4-ebe1-41e2-8d60-d0305ef40ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "warmup = 10\n",
    "num_epochs = 100\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9764224d-c5f7-4811-bc90-95a7e52d2ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = INRModel(\n",
    "    net,\n",
    "    reg_pde=reg_loss,\n",
    "    learning_rate=learning_rate,\n",
    "    warmup=warmup,\n",
    "    num_epochs=num_epochs,\n",
    "    alpha=1e-4,\n",
    "    Lr=1.0,\n",
    "    f=1.0,\n",
    "    g=1.0,\n",
    "    qg=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b94dae0-3f14-471b-bb82-5206b862d5ad",
   "metadata": {},
   "source": [
    "### Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc58be52-50e3-4072-82c4-7ef17fb24fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    min_epochs=1,\n",
    "    max_epochs=num_epochs,\n",
    "    accelerator=\"mps\",\n",
    "    # devices=1,\n",
    "    enable_progress_bar=True,\n",
    "    # logger=wandb_logger,\n",
    "    callbacks=callbacks,\n",
    "    # accumulate_grad_batches=10\n",
    "    # gradient_clip_val=1.0,\n",
    "    # gradient_clip_algorithm=\"norm\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbd7117-e4db-426f-b470-bfb22c424570",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0889c892-20c6-4446-8818-f8b7a6df9e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(\n",
    "    learn,\n",
    "    datamodule=dm,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e87e6b4-1d97-4d5d-a00b-2c14b28677e0",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b7dc94-9401-46b7-99dd-9643352951e0",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ceb782e-493c-43cb-a481-9a36bdf33045",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = trainer.test(learn, dataloaders=dm.test_dataloader())\n",
    "\n",
    "# results[\"data\"] = res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e93692-f60c-457a-b483-eb7a8da4d771",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b77091-4ca8-4084-9bc3-a2eb75ae545d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# t0 = time.time()\n",
    "predictions = trainer.predict(learn, dataloaders=dm, return_predictions=True)\n",
    "predictions = torch.cat(predictions)\n",
    "# t1 = time.time() - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e050c0e-a4a7-4385-a9a0-69f76769223f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_pred = dm.create_predictions_ds(predictions)\n",
    "ds_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c07d577-c1dc-4e4a-bf74-3fcf4238671c",
   "metadata": {},
   "source": [
    "### Figure I: Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371074d9-6e15-4cc8-9a34-d5df99116f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_pred.pred.thin(time=1).plot.imshow(\n",
    "    col=\"time\",\n",
    "    robust=True,\n",
    "    col_wrap=3,\n",
    "    cmap=\"viridis\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716c20d6-d0f9-413e-8d11-00f35fae2a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_pred.pred.hvplot.image(x=\"Nx\", y=\"Ny\", width=500, height=400, cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2f6335-81a6-4ceb-949b-2662225e8a7b",
   "metadata": {},
   "source": [
    "### Figure II: Ground Truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdb20ec-9944-4fb2-b531-ebe0f1277dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_pred.true.thin(time=1).plot.imshow(\n",
    "    col=\"time\",\n",
    "    robust=True,\n",
    "    col_wrap=3,\n",
    "    cmap=\"viridis\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98ef878-2f4f-4f9e-9249-049b5ac27a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_pred.true.hvplot.image(x=\"Nx\", y=\"Ny\", width=500, height=400, cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0887d598-dcc4-4691-a187-f8f334f120f8",
   "metadata": {},
   "source": [
    "### Figure III: Absolute Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc5262c-2b6e-44e2-a5eb-a49d58ddcf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "(ds_pred.true - ds_pred.pred).thin(time=1).plot.imshow(\n",
    "    col=\"time\",\n",
    "    robust=True,\n",
    "    col_wrap=3,\n",
    "    cmap=\"RdBu_r\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f73aa0-204b-40b3-9afc-c74803dff47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(ds_pred.true - ds_pred.pred).hvplot.image(\n",
    "    x=\"Nx\", y=\"Ny\", width=500, height=400, cmap=\"viridis\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6332052",
   "metadata": {},
   "source": [
    "# PINN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7302cce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def laplace(y, x):\n",
    "    grad = gradient(y, x)\n",
    "    return divergence(grad, x)\n",
    "\n",
    "\n",
    "def divergence(y, x):\n",
    "    div = 0.0\n",
    "    for i in range(y.shape[-1]):\n",
    "        div += torch.autograd.grad(\n",
    "            y[..., i], x, torch.ones_like(y[..., i]), create_graph=True\n",
    "        )[0][..., i : i + 1]\n",
    "    return div\n",
    "\n",
    "\n",
    "def gradient(y, x, grad_outputs=None):\n",
    "    if grad_outputs is None:\n",
    "        grad_outputs = torch.ones_like(y)\n",
    "    grad = torch.autograd.grad(y, [x], grad_outputs=grad_outputs, create_graph=True)[0]\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b91e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ydata = data.p.values[:nbmax, :ss, :ss].flatten()[:, None]\n",
    "# ydataT = torch.from_numpy(ydata)#.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c70ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata.shape, ydata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba8c24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mseloss = torch.nn.MSELoss()\n",
    "\n",
    "\n",
    "def PINNloss(pred_p, lab, coords, w=1e-4):  # 1e-4\n",
    "    grad_nn = gradient(pred_p, coords)\n",
    "    q_nn = divergence(grad_nn, coords)\n",
    "    dlaplacU = gradient(q_nn, coords)\n",
    "    Jacob_U_laplacU = grad_nn[:, 1] * dlaplacU[:, 2] - grad_nn[:, 2] * dlaplacU[:, 1]\n",
    "    ###########\n",
    "    function_loss = mseloss(dlaplacU[:, 0] + Jacob_U_laplacU, torch.zeros(1).double())\n",
    "    data_loss = mseloss(pred_p, lab)\n",
    "    return data_loss + w * function_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6370d5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = np.random.permutation(len(traindata))\n",
    "nbsample = np.int64(len(traindata) * 0.1)  # 1% of data\n",
    "nbsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528eb7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loader = torch.utils.data.DataLoader(\n",
    "    list(zip(traindata[perm, :][:nbsample, :], ydata[perm, :][:nbsample, :])),\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7374c29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model instantiation\n",
    "torch.manual_seed(2022)\n",
    "\n",
    "model = Siren(\n",
    "    in_features=3,\n",
    "    out_features=1,\n",
    "    hidden_features=64,\n",
    "    hidden_layers=3,\n",
    "    outermost_linear=True,\n",
    ").double()\n",
    "# model.load_state_dict(torch.load('/home/rlguensat/MLstuff/SIREN_addPhysLoss_SGD.h5'))#_finetune_addPhysLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2a0124",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(lr=1e-4, params=model.parameters())\n",
    "\n",
    "\n",
    "def train_one_epoch():\n",
    "    running_loss = 0.0\n",
    "    last_loss = 0.0\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(training_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs, coords = model(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = PINNloss(outputs, labels, coords)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # epoch loss\n",
    "        epoch_loss += outputs.shape[0] * loss.item()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:\n",
    "            last_loss = running_loss / 1000  # print every 1000 mini-batch\n",
    "            print(\"  batch {} loss: {}\".format(i + 1, last_loss))\n",
    "            running_loss = 0.0\n",
    "\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb655a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_adam = []\n",
    "\n",
    "EPOCHS = 100\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"EPOCH {}:\".format(epoch + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    epoch_loss = train_one_epoch()\n",
    "    h_adam.append(epoch_loss)\n",
    "    print(\"EPOCH loss {}:\".format(epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507858db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16\n",
    "plt.semilogy(h_adam, label=\"Adam\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d89722",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), '/home/rlguensat/MLstuff/SIREN_addPhysLoss_noweighting.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ac2fd3",
   "metadata": {},
   "source": [
    "# Let us take the first image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6ad719",
   "metadata": {},
   "outputs": [],
   "source": [
    "point = torch.from_numpy(traindata[: ss * 256, :])\n",
    "point.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a071b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8452172",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "pred_p, cc = model(point)\n",
    "grad_nn = gradient(pred_p, cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f63da09",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_nn = divergence(grad_nn, cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d641b4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(pred_p.detach().numpy().reshape((256, ss)))\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(data.p[0, :256, :ss])\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(\n",
    "    pred_p.detach().numpy().reshape((256, ss)) - data.p[0, :256, :ss], cmap=\"coolwarm\"\n",
    ")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413143e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow((grad_nn[:, 1]).detach().numpy().reshape((256, ss)))\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(data.v[0, :256, :ss])\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(\n",
    "    (grad_nn[:, 1]).detach().numpy().reshape((256, ss)) - data.v[0, :256, :ss],\n",
    "    cmap=\"coolwarm\",\n",
    ")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c9744f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow((q_nn[:, 0]).detach().numpy().reshape((256, ss)), vmin=-2, vmax=2)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(data.q[0, :256, :ss], vmin=-2, vmax=2)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(\n",
    "    (q_nn[:, 0]).detach().numpy().reshape((256, ss)) - data.q[0, :256, :ss],\n",
    "    cmap=\"coolwarm\",\n",
    ")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd813629",
   "metadata": {},
   "source": [
    "# Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206f5a58",
   "metadata": {},
   "source": [
    "$$  \\partial_t q + \\boldsymbol{J}(p, q)= 0  $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aecf1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dlaplacU = gradient(q_nn, cc)\n",
    "Jacob_U_laplacU = grad_nn[:, 1] * dlaplacU[:, 2] - grad_nn[:, 2] * dlaplacU[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83c0c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist((dlaplacU[:, 0] + Jacob_U_laplacU).detach().numpy(), bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ea522d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch_py39]",
   "language": "python",
   "name": "conda-env-torch_py39-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
