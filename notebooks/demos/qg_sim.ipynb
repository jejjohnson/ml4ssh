{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c00d4b7-e07e-4405-b643-c4db64f82510",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# QG Simulation\n",
    "\n",
    "* Fourier Feature Networks (FFN)\n",
    "* Siren\n",
    "* Modulated Siren (ModSiren)\n",
    "* Multiplicative Filter Networks (MFN)\n",
    "    * Fourier\n",
    "    * Gabor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a2ee9e-04cc-4c5e-a3eb-4742a7ad7cd1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "from pyprojroot import here\n",
    "\n",
    "# spyder up to find the root\n",
    "root = here(project_files=[\".root\"])\n",
    "local = root.joinpath(\"experiments/qg\")\n",
    "\n",
    "# append to path\n",
    "sys.path.append(str(root))\n",
    "sys.path.append(str(local))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43eaf2f-23b0-4937-80a3-21938dc73ed2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Dict, Any, cast\n",
    "import tabulate\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.nn import ReLU\n",
    "\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "import os, imageio\n",
    "\n",
    "from inr4ssh._src.models.mlp import MLP\n",
    "from inr4ssh._src.models.activations import Swish\n",
    "from inr4ssh._src.datamodules.qg import QGSimulation\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from inr4ssh._src.models.image import ImageModel\n",
    "from inr4ssh._src.models.siren import Siren, SirenNet, Modulator, ModulatedSirenNet\n",
    "from inr4ssh._src.models.mfn import FourierNet, GaborNet\n",
    "from inr4ssh._src.models.activations import get_activation\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import TQDMProgressBar\n",
    "from pytorch_lightning.callbacks.model_checkpoint import ModelCheckpoint\n",
    "from pytorch_lightning.utilities.argparse import add_argparse_args\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "import hvplot.xarray\n",
    "\n",
    "pl.seed_everything(123)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import wandb\n",
    "\n",
    "sns.reset_defaults()\n",
    "sns.set_context(context=\"talk\", font_scale=0.7)\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fc5b29-5204-4e91-acd9-ec85019cb687",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a180a4-628d-4bc4-affe-9bd58bb28b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "from simple_parsing import ArgumentParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8578ca-f93e-4cdd-ba8e-292bb6e9d94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize argparse\n",
    "parser = ArgumentParser()\n",
    "\n",
    "# add all experiment arguments\n",
    "parser.add_arguments(config.Logging, dest=\"logging\")\n",
    "parser.add_arguments(config.DataDir, dest=\"data\")\n",
    "parser.add_arguments(config.PreProcess, dest=\"preprocess\")\n",
    "parser.add_arguments(config.Features, dest=\"features\")\n",
    "parser.add_arguments(config.TrainTestSplit, dest=\"traintest\")\n",
    "parser.add_arguments(config.DataLoader, dest=\"dataloader\")\n",
    "parser.add_arguments(config.Model, dest=\"model\")\n",
    "parser.add_arguments(config.Siren, dest=\"siren\")\n",
    "parser.add_arguments(config.MLP, dest=\"mlp\")\n",
    "parser.add_arguments(config.FFN, dest=\"ffn\")\n",
    "parser.add_arguments(config.ModulatedSiren, dest=\"modsiren\")\n",
    "parser.add_arguments(config.MFN, dest=\"mfn\")\n",
    "parser.add_arguments(config.Losses, dest=\"losses\")\n",
    "parser.add_arguments(config.Optimizer, dest=\"optimizer\")\n",
    "parser.add_arguments(config.LRScheduler, dest=\"lr_scheduler\")\n",
    "parser.add_arguments(config.Callbacks, dest=\"callbacks\")\n",
    "# parser.add_arguments(config.EvalData, dest=\"eval\")\n",
    "# parser.add_arguments(config.Metrics, dest=\"metrics\")\n",
    "# parser.add_arguments(config.Viz, dest=\"viz\")\n",
    "\n",
    "# parse args\n",
    "args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee75248-815f-405f-8dac-9c061e0c25e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "args.dataloader.batch_size = 4096  # 8192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022e53dc-a678-4abe-922f-798d9c9021ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# args.data.data_dir = f\"/Users/eman/.CMVolumes/cal1_workdir/data/qg_data/public/\"\n",
    "args.data.data_dir = f\"/Volumes/EMANS_HDD/data/qg_sim/\"\n",
    "args.logging.log_dir = \"~/code_projects/logs/\"\n",
    "args.logging.mode = \"online\"\n",
    "\n",
    "args.traintest.noise = None\n",
    "\n",
    "args.traintest.step_Nx = 1\n",
    "args.traintest.step_Ny = 1\n",
    "args.traintest.step_time = 1\n",
    "args.traintest.missing_data = 0.5\n",
    "\n",
    "args.model = \"siren\"\n",
    "model_config = args.siren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664d10c6-f915-4f3c-9ee2-a6b4d2fc769e",
   "metadata": {},
   "source": [
    "## Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525c4179-3fae-4a30-978d-f0da657cd897",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inr4ssh._src.io import simpleargs_2_ndict\n",
    "\n",
    "log_options = args.logging\n",
    "\n",
    "# params_dict = simpleargs_2_ndict(args)\n",
    "\n",
    "wandb_logger = WandbLogger(\n",
    "    config=args,\n",
    "    mode=log_options.mode,\n",
    "    project=log_options.project,\n",
    "    entity=log_options.entity,\n",
    "    dir=log_options.log_dir,\n",
    "    resume=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9c836a-8790-4f6c-b5c2-4af553875648",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bddc4c-039a-4c63-87b5-195c44259468",
   "metadata": {},
   "outputs": [],
   "source": [
    "config.DataDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7a614f-916d-4bc9-a93f-00f6d95b6c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from inr4ssh._src.data.qg import load_qg_data\n",
    "\n",
    "# ds = load_qg_data(dm.data.data_dir)\n",
    "\n",
    "# ds = ds.coarsen({\"time\": 2}, boundary=\"trim\", coord_func=\"mean\")\n",
    "# ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7855355e-1c69-41cc-b0c5-c6b07602f21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = QGSimulation(\n",
    "    data=args.data,\n",
    "    preprocess=args.preprocess,\n",
    "    traintest=args.traintest,\n",
    "    features=args.features,\n",
    "    dataloader=args.dataloader,\n",
    "    # eval=args.eval\n",
    ")\n",
    "\n",
    "dm.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8fe535-71e6-4f1f-9d54-688177da26f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dm.ds_train), len(dm.ds_valid), len(dm.ds_test), len(dm.ds_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3834af2-1b4d-4938-9262-f9128bd8e920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "# def array_2_da(coords, data, name=\"full_pred\", coords_name: List[str]=[\"x\", \"y\", \"t\"]):\n",
    "#     return pd.DataFrame(np.hstack([coords, data]), columns=[coords_name]+[name]).set_index(\n",
    "#         coords_name).to_xarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da42420-5107-4cb3-9299-883a40ac4bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "xr_data = dm.create_xr_dataset(\"predict\")\n",
    "xr_data = xr.merge([xr_data, dm.create_xr_dataset(\"train\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea6cdde-2843-44c4-8354-735f3a455352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xr_data.train.hvplot.image(x=\"Nx\", y=\"Ny\", width=500, height=400, cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bf9c29-368d-431b-b4b5-3bf1802d71b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xr_data.predict.hvplot.image(x=\"Nx\", y=\"Ny\", width=500, height=400, cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c86ed21-0ae8-4682-ad36-38da4d2db3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.obs.thin(time=2).plot.imshow(\n",
    "#     col=\"time\", robust=True, col_wrap=4, cmap=\"viridis\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a850a1a-760a-45c0-9edb-7380fe7dbe87",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The input data is a coordinate vector, $\\mathbf{x}_\\phi$, of the image coordinates.\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_\\phi \\in \\mathbb{R}^{D_\\phi}\n",
    "$$\n",
    "\n",
    "where $D_\\phi = [\\text{x}, \\text{y}]$. So we are interested in learning a function, $\\boldsymbol{f}$, such that we can input a coordinate vector and output a scaler/vector value of the pixel value.\n",
    "\n",
    "$$\n",
    "\\mathbf{u} = \\boldsymbol{f}(\\mathbf{x}_\\phi; \\boldsymbol{\\theta})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccc0de5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Data Module\n",
    "\n",
    "\n",
    "#### Train-Test Split\n",
    "\n",
    "In this example, we are only taking every other pixel for training and validation. It is a very simple and well-defined problem which each of the neural networks should be able to solve. The final test image is the original full resolution image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c54fc3-f3b5-45f4-b68f-15192bbfc605",
   "metadata": {},
   "source": [
    "Notice how we have `131_072` points from training and validation and `262_144` for the testing. This is because we have *raveled* the image where each coordinate is a vector of `x,y`. So these are a lot of points..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fcb801-d1e5-4f3b-88a1-ec4580b29162",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "init = dm.ds_train[:32]\n",
    "x_init, y_init = init\n",
    "x_init.shape, y_init.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18098026-febf-44a1-91cf-5a34e2417a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_init.min(), x_init.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255b934c-ab01-4818-8ecb-cbfab71a9192",
   "metadata": {},
   "source": [
    "### Optimizer\n",
    "\n",
    "For this, we will use a simple adam optimizer with a `learning_rate` of 1e-4. From many studies, it appears that a lower learning rate works well with this methods because there is a lot of data. In addition, a bigger `batch_size` is also desireable. We will set the `num_epochs` to `1_000` which should be good enough for a single image. Obviously more epochs and a better learning rate scheduler would result in better results but this will be sufficient for this demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb2b217-a6b4-4210-97b5-2f00a0cefc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 300\n",
    "learning_rate = 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f9b10d-159a-4d90-9a3d-a893a5b7d87d",
   "metadata": {},
   "source": [
    "### Scheduler\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"http://www.bdhammel.com/assets/learning-rate/resnet_loss.png\" alt=\"drawing\" width=\"300\"/>\n",
    "<figcaption align = \"center\">\n",
    "  <b>Fig.1 - An example for learning rate reduction when the validation loss stagnates. Source: \n",
    "    <a href=\"http://www.bdhammel.com/assets/learning-rate/resnet_loss.png\">Blog</a>\n",
    "  </b>\n",
    "  </figcaption>\n",
    "</p>\n",
    "\n",
    "We will use a simple learning rate scheduler - `reduce_lr_on_plateau`. This will automatically reduce the learning rate as the validation loss stagnates. It will ensure that we really squeeze out as much performance as possible from our models during the training procedure.We start with a (relatively) high `learning_rate` of `1e-4` so we will set the `patience` to 5 epochs. So if there is no change in with every epoch, we decrease the learning rate by a factor of `0.1`.\n",
    "\n",
    "This is a rather crude (but effective) method but it tends to work well in some situations. A better method might be the `cosine_annealing` method or the `exponential_decay` method. See other [examples](https://www.kaggle.com/code/snnclsr/learning-rate-schedulers/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd7a402-e645-4c97-81ac-c8658a8a56f9",
   "metadata": {},
   "source": [
    "### Loss\n",
    "\n",
    "We are going with a very simple `loss` function: the *mean squared error* (MSE). This is given by:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\boldsymbol{\\theta}) = \\frac{1}{N} \\sum_n^N \\left( \\mathbf{y}_n - \\boldsymbol{f}_{\\boldsymbol{\\theta}}(\\mathbf{x}_n) \\right)^2\n",
    "$$\n",
    "\n",
    "We won't code this from scratch, we will just use the PyTorch function, `nn.MSELoss`, and we will use the `mean` reduction parameter.\n",
    "\n",
    "\n",
    "### PSNR\n",
    "\n",
    "We will also keep track of the signal to noise ratio (PSNR) which will give us an indication of how well we are learning.\n",
    "\n",
    "$$\n",
    "\\text{PSNR}(\\mathbf{x}) = - 10 \\log (2 * \\text{MSE}(\\mathbf{x}))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe9971a-8108-4e2e-ae58-5e412c6c27a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc78bc6-8540-44d1-8ec2-3e08c8ad1ef6",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa1e241-c529-49c8-a832-0760f9847d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps_per_epoch = len(dm.train_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a704a935-9d80-4476-95db-60a34b3202ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pl_bolts.optimizers.lr_scheduler import LinearWarmupCosineAnnealingLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bebfb9-2b68-400c-8af9-5f2d83cd6a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageModel(pl.LightningModule):\n",
    "    def __init__(self, model, optimizer: str = \"adam\", **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "        self.model = model\n",
    "        self.hyperparams = cast(Dict[str, Any], self.hparams)\n",
    "        self.loss = nn.MSELoss(reduction=\"mean\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # output\n",
    "        x, y = batch\n",
    "\n",
    "        pred = self.forward(x)\n",
    "\n",
    "        # loss function\n",
    "        loss = self.loss(y, pred)\n",
    "\n",
    "        self.log(\"train_loss\", loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # output\n",
    "        x, y = batch\n",
    "\n",
    "        pred = self.forward(x)\n",
    "\n",
    "        # loss function\n",
    "        loss = self.loss(y, pred)\n",
    "\n",
    "        self.log(\"val_loss\", loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # output\n",
    "        x, y = batch\n",
    "\n",
    "        pred = self.forward(x)\n",
    "\n",
    "        # loss function\n",
    "        loss = self.loss(y, pred)\n",
    "\n",
    "        self.log(\"test_loss\", loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        # output\n",
    "        x, y = batch\n",
    "\n",
    "        pred = self.forward(x)\n",
    "\n",
    "        return pred\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        # configure optimizer\n",
    "        if self.hyperparams.optimizer == \"adam\":\n",
    "            optimizer = Adam(\n",
    "                self.model.parameters(), lr=self.hyperparams.get(\"lr\", 1e-4)\n",
    "            )\n",
    "        elif self.hyperparams.optimizer == \"lbfgs\":\n",
    "            optimizer = Adam(\n",
    "                self.model.parameters(), lr=self.hyperparams.get(\"lr\", 1e-4)\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unrecognized optimizer: {optimizer}\")\n",
    "\n",
    "        # configure lr scheduler\n",
    "        # scheduler = ReduceLROnPlateau(\n",
    "        #     optimizer, patience=self.hyperparams.get(\"lr_schedule_patience\", 5)\n",
    "        # )\n",
    "        # scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "        #     optimizer,\n",
    "        #     max_lr=self.hyperparams.get(\"lr\", 1e-4),\n",
    "        #     steps_per_epoch=num_steps_per_epoch,\n",
    "        #     epochs=num_epochs,\n",
    "        # )\n",
    "        # scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        #     optimizer, T_max=num_epochs, eta_min=0\n",
    "        # )\n",
    "        scheduler = LinearWarmupCosineAnnealingLR(\n",
    "            optimizer,\n",
    "            warmup_epochs=50,\n",
    "            max_epochs=num_epochs,\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": scheduler,\n",
    "            \"monitor\": \"val_loss\",\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6220c818-62ab-45fa-b104-914eb6a47f5e",
   "metadata": {},
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e4ca869-16be-426e-a358-a05c0208209a",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [TQDMProgressBar(refresh_rate=5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025da5c8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model\n",
    "\n",
    "* Fourier Feature Networks (FFN)\n",
    "* Siren\n",
    "* Modulated Siren (ModSiren)\n",
    "* Multiplicative Filter Networks (MFN)\n",
    "    * Fourier\n",
    "    * Gabor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c106a352-bcbc-41f9-9f0a-31ec78de0129",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inr4ssh._src.models.models_factory import model_factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acbeacf-623e-4e7f-a781-fd24e59acb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_in = x_init.shape[1]\n",
    "dim_out = y_init.shape[1]\n",
    "\n",
    "\n",
    "# args.ffn.encoder = \"positional\"\n",
    "net = model_factory(args.model, dim_in, dim_out, model_config)\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2fac68-f6af-4c25-8caa-2c9ce0a2557f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# dim_in = x_init.shape[1]\n",
    "# dim_hidden = 256\n",
    "# dim_out = y_init.shape[1]\n",
    "# num_layers = 5\n",
    "# activation = \"swish\"  # Swish()  # nn.ReLU()#\n",
    "# final_activation = \"identity\"\n",
    "\n",
    "# mlp_net = MLP(\n",
    "#     dim_in=dim_in,\n",
    "#     dim_hidden=dim_hidden,\n",
    "#     dim_out=dim_out,\n",
    "#     num_layers=num_layers,\n",
    "#     activation=get_activation(activation),\n",
    "#     final_activation=get_activation(final_activation),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d88b6fc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "learn = ImageModel(net, learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbb96b4-6d83-47f0-92b7-42337db88522",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = learn.forward(x_init)\n",
    "\n",
    "# assert out.shape[0] == x_init.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36470708-94b0-4d1f-a41f-37a5dfe30c7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12e506e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    min_epochs=1,\n",
    "    max_epochs=num_epochs,\n",
    "    accelerator=\"mps\",\n",
    "    # devices=1,\n",
    "    enable_progress_bar=True,\n",
    "    logger=wandb_logger,\n",
    "    callbacks=callbacks,\n",
    "    # gradient_clip_val=1.0,\n",
    "    # gradient_clip_algorithm=\"norm\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27eb75f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "trainer.fit(\n",
    "    learn,\n",
    "    datamodule=dm,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecaf881",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "res = trainer.test(learn, dataloaders=dm.test_dataloader())\n",
    "\n",
    "results[\"mlp\"] = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227a0da9-8545-4329-ad65-d1071685fc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = [\n",
    "    [\n",
    "        key,\n",
    "        f\"{results[key][0]['test_loss']:4.4f}\",\n",
    "        # \"{:,}\".format(sum([np.prod(p.shape) for p in flow_dict[key][\"model\"].parameters()]))\n",
    "    ]\n",
    "    for key in results\n",
    "]\n",
    "display(\n",
    "    HTML(\n",
    "        tabulate.tabulate(\n",
    "            table,\n",
    "            tablefmt=\"html\",\n",
    "            headers=[\n",
    "                \"Model\",\n",
    "                \"MSE\",  # \"Num Parameters\"\n",
    "            ],\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ded860",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# t0 = time.time()\n",
    "predictions = trainer.predict(learn, dataloaders=dm, return_predictions=True)\n",
    "predictions = torch.cat(predictions)\n",
    "# t1 = time.time() - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151339ed",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ds_pred = dm.create_predictions_ds(predictions)\n",
    "ds_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09b8831-25b5-4680-b32c-abbb72d5968b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_pred.pred.thin(time=2).plot.imshow(\n",
    "    col=\"time\",\n",
    "    robust=True,\n",
    "    col_wrap=4,\n",
    "    cmap=\"viridis\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f911f61a-ec94-4c0e-9d14-3b06e0df6816",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_pred.true.thin(time=2).plot.imshow(\n",
    "    col=\"time\",\n",
    "    robust=True,\n",
    "    col_wrap=4,\n",
    "    cmap=\"viridis\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4b19ba-cfd3-4cf8-9ab9-f24c0d45ec33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645b8ad8-9ddf-4988-81b3-530ec95bf2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_pred.pred.hvplot.image(x=\"Nx\", y=\"Ny\", width=500, height=400, cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cbf00c-f5cd-4cb1-8461-96c3dd48e617",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_pred.true.hvplot.image(x=\"Nx\", y=\"Ny\", width=500, height=400, cmap=\"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fc461e-91c9-4ca5-9c8d-e42917b66cf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "70d2e25024e70c662658c456d9e0a340af3759493bed99b867179c40cf269d86"
  },
  "kernelspec": {
   "display_name": "Python [conda env:torch_py39]",
   "language": "python",
   "name": "conda-env-torch_py39-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
