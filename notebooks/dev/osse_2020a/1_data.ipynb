{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from pyprojroot import here\n",
    "\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "\n",
    "# spyder up to find the root\n",
    "root = here(project_files=[\".root\"])\n",
    "exp = here(\n",
    "    relative_project_path=root.joinpath(\"experiments/dc21a\"), project_files=[\".local\"]\n",
    ")\n",
    "\n",
    "\n",
    "# append to path\n",
    "sys.path.append(str(root))\n",
    "sys.path.append(str(exp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import Compose\n",
    "import xarray as xr\n",
    "from inr4ssh._src.datasets import AlongTrackDataset\n",
    "from inr4ssh._src.datasets.utils import (\n",
    "    TimeJulianMinMax,\n",
    "    TimeJulian,\n",
    "    TimeMinMax,\n",
    "    ToTensor,\n",
    "    get_num_training,\n",
    ")\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "spatial_columns = [\"lon\", \"lat\"], [\"x\", \"y\", \"z\"], [\"lon_rad\", \"lat_rad\"]\n",
    "temporal_columns = [\"time\"], [\"vtime\"]\n",
    "```\n",
    "\n",
    "**Transformations**\n",
    "\n",
    "* Spherical 2 Cartesian\n",
    "* Spherical Degrees to Radians\n",
    "* Temporal to Julian\n",
    "* TimeStamps 2 Days of the Year\n",
    "* TimeStamps 2 Cycles\n",
    "* Temporal Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_link = \"/Volumes/EMANS_HDD/data/dc20a_osse/test/ml/nadir1.nc\"\n",
    "\n",
    "ds = xr.open_dataset(ds_link)\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Transforms\n",
    "\n",
    "So there are a few transformations we should do within the dataset: 1) timestamps and 2) torch tensors. In general, the xarray datasets will almost always have numpy arrays for the spatial and output values. So we need to change them into torch tensors. We also have datetime64 data structures for the time values. So we need to transform those into numerical values and additionally into torch tensors.\n",
    "\n",
    "**Note**: There are other additional transformations we can do, e.g. spherical, cartesian, etc, but I decided to offload them to the `trainer` (which will be discussed later). In general, the dataset transformations should only have transformations that change numpy arrays to torch tensors \n",
    "\n",
    "There are some available transformations within the library:\n",
    "* Julian Time (Temporal Transform)\n",
    "* TimeMinMax (Temporal Transform)\n",
    "* ToTensor (Spatial, Temporal, Output Transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define column names\n",
    "spatial_columns = [\"lon\", \"lat\"]\n",
    "temporal_columns = [\"time\"]\n",
    "output_columns = [\"ssh_model\"]\n",
    "\n",
    "# initialize dataset\n",
    "torch_ds = AlongTrackDataset(\n",
    "    ds, spatial_columns, temporal_columns, output_columns, transform=None\n",
    ")\n",
    "\n",
    "batchsize = 32\n",
    "\n",
    "ibatch = torch_ds[0:batchsize]\n",
    "\n",
    "ibatch[\"spatial\"].shape, ibatch[\"temporal\"].shape, ibatch[\"output\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define column names\n",
    "spatial_columns = [\"lon\", \"lat\"]\n",
    "temporal_columns = [\"time\"]\n",
    "output_columns = [\"ssh_model\"]\n",
    "\n",
    "# initialize dataset\n",
    "torch_ds = AlongTrackDataset(\n",
    "    ds, spatial_columns, temporal_columns, output_columns=None, transform=None\n",
    ")\n",
    "\n",
    "batchsize = 32\n",
    "\n",
    "ibatch = torch_ds[0:batchsize]\n",
    "\n",
    "ibatch[\"spatial\"].shape, ibatch[\"temporal\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Compose(\n",
    "    [\n",
    "        # TimeMinMax(),\n",
    "        # TimeJulian(),\n",
    "        TimeJulianMinMax(),\n",
    "        ToTensor(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize dataset\n",
    "torch_ds = AlongTrackDataset(\n",
    "    ds, spatial_columns, temporal_columns, output_columns, transform=transform\n",
    ")\n",
    "\n",
    "batchsize = 32\n",
    "\n",
    "ibatch = torch_ds[0:batchsize]\n",
    "\n",
    "ibatch[\"spatial\"].shape, ibatch[\"temporal\"].shape, ibatch[\"output\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize dataset\n",
    "torch_ds = AlongTrackDataset(\n",
    "    ds, spatial_columns, temporal_columns, output_columns=None, transform=transform\n",
    ")\n",
    "\n",
    "batchsize = 32\n",
    "\n",
    "ibatch = torch_ds[0:batchsize]\n",
    "\n",
    "ibatch[\"spatial\"].shape, ibatch[\"temporal\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ibatch[\"temporal\"].min(), ibatch[\"temporal\"].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility: `pd.DataFrame`, `xr.Dataset`\n",
    "\n",
    "For inference/predictions, we will often have a dataset we want for predictions, and then we want to extract a dataframe and/or dataset with all of the coordinates. These utility functions will help do that using the attributes within the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize dataset\n",
    "torch_ds = AlongTrackDataset(\n",
    "    ds, spatial_columns, temporal_columns, output_columns, transform=transform\n",
    ")\n",
    "\n",
    "outputs = torch_ds[:][\"output\"]\n",
    "\n",
    "ds_ = torch_ds.create_predict_df(outputs)\n",
    "\n",
    "ds_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize dataset\n",
    "torch_ds = AlongTrackDataset(\n",
    "    ds, spatial_columns, temporal_columns, output_columns=None, transform=transform\n",
    ")\n",
    "\n",
    "\n",
    "ds_ = torch_ds.create_predict_df(outputs)\n",
    "\n",
    "ds_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_ = torch_ds.create_predict_ds(outputs)\n",
    "ds_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_prct = 0.9\n",
    "num_train, num_valid = get_num_training(len(torch_ds), train_prct=train_prct)\n",
    "train_split_seed = 42\n",
    "\n",
    "train_set, valid_set = torch.utils.data.random_split(\n",
    "    torch_ds,\n",
    "    [num_train, num_valid],\n",
    "    generator=torch.Generator().manual_seed(train_split_seed),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader\n",
    "\n",
    "So finally, we can easily put this in a dataloader. This makes things really easy in terms for generating batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "valid_dl = torch.utils.data.DataLoader(valid_set, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ibatch = next(iter(torch_dl))\n",
    "for ibatch in train_dl:\n",
    "    break\n",
    "\n",
    "for ivbatch in valid_dl:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ibatch[\"spatial\"].shape, ibatch[\"temporal\"].shape  # , ibatch[\"output\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ivbatch[\"spatial\"].shape, ivbatch[\"temporal\"].shape  # , ivbatch[\"output\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataModule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Configs**\n",
    "\n",
    "* `data` - the dataset directory and setup arguments, e.g. data_dir\n",
    "* `preprocess` - the preprocessing arguments, e.g. spatial/temporal subset, coarsening\n",
    "* `traintest` - the train/valid split arguments, e.g. train_prct\n",
    "* `dataloader` - the dataloader arguments, e.g. batchsize, num_workers\n",
    "* `evaluation` - the evaluation grid for the prediction dataloader, e.g. lon/lon bnds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from inr4ssh._src.datamodules.utils import transform_factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_collections import config_dict\n",
    "\n",
    "\n",
    "def get_demo_config():\n",
    "    config = config_dict.ConfigDict()\n",
    "\n",
    "    # data directory\n",
    "    config.data = data = config_dict.ConfigDict()\n",
    "    data.dataset_dir = \"/Volumes/EMANS_HDD/data/dc20a_osse/test/ml/nadir1.nc\"\n",
    "\n",
    "    # preprocessing\n",
    "    config.preprocess = preprocess = config_dict.ConfigDict()\n",
    "    preprocess.preprocess = False\n",
    "\n",
    "    # train/valid arguments\n",
    "    config.traintest = traintest = config_dict.ConfigDict()\n",
    "    traintest.train_prct = 0.9\n",
    "    traintest.seed = 42\n",
    "\n",
    "    # dataloader\n",
    "    config.dataloader = dataloader = config_dict.ConfigDict()\n",
    "    # train dataloader\n",
    "    dataloader.batchsize_train = 32\n",
    "    dataloader.num_workers_train = 4\n",
    "    dataloader.shuffle_train = True\n",
    "    dataloader.pin_memory_train = True\n",
    "    # valid dataloader\n",
    "    dataloader.batchsize_valid = 32\n",
    "    dataloader.num_workers_valid = 4\n",
    "    dataloader.shuffle_valid = False\n",
    "    dataloader.pin_memory_valid = True\n",
    "    # predict dataloader\n",
    "    dataloader.batchsize_predict = 32\n",
    "    dataloader.num_workers_predict = 4\n",
    "    dataloader.shuffle_predict = False\n",
    "    dataloader.pin_memory_predict = True\n",
    "\n",
    "    # EVALUATION\n",
    "    config.evaluation = evaluation = config_dict.ConfigDict()\n",
    "    evaluation.lon_min = 35.0\n",
    "    evaluation.lon_max = 35.0\n",
    "    evaluation.lat_min = 35.0\n",
    "    evaluation.lat_max = 35.0\n",
    "    evaluation.time_min = \"1900-10-10\"\n",
    "    evaluation.time_max = \"2100-10-10\"\n",
    "\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loguru import logger\n",
    "from inr4ssh._src.datasets.utils import (\n",
    "    TimeJulianMinMax,\n",
    "    TimeJulian,\n",
    "    TimeMinMax,\n",
    "    ToTensor,\n",
    "    get_num_training,\n",
    ")\n",
    "\n",
    "\n",
    "class AlongTrackDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data, preprocess, traintest, dataloader, eval):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.preprocess = preprocess\n",
    "        self.traintest = traintest\n",
    "        self.dataloader = dataloader\n",
    "        self.eval = eval\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "\n",
    "        # open xarray dataset\n",
    "        logger.info(f\"Opening xarray dataset...\")\n",
    "        logger.info(f\"Dataset: {self.data.dataset_dir}\")\n",
    "\n",
    "        ds = xr.open_dataset(self.data.dataset_dir)\n",
    "\n",
    "        # TODO: add preprocessing step\n",
    "        if self.preprocess:\n",
    "            if self.preprocess.preprocess:\n",
    "                raise NotImplementedError()\n",
    "\n",
    "        # create dataset\n",
    "        logger.info(f\"Creating dataset...\")\n",
    "        ds = AlongTrackDataset(\n",
    "            ds=ds,\n",
    "            spatial_columns=[\"lon\", \"lat\"],\n",
    "            temporal_columns=[\"time\"],\n",
    "            output_columns=[\"ssh_model\"],\n",
    "        )\n",
    "\n",
    "        # train/test split\n",
    "        logger.info(f\"Train/Valid Split ({self.traintest.train_prct*100}%)...\")\n",
    "        num_train, num_valid = get_num_training(\n",
    "            len(torch_ds), train_prct=self.traintest.train_prct\n",
    "        )\n",
    "\n",
    "        train_set, valid_set = torch.utils.data.random_split(\n",
    "            torch_ds,\n",
    "            [num_train, num_valid],\n",
    "            generator=torch.Generator().manual_seed(self.traintest.seed),\n",
    "        )\n",
    "\n",
    "        # train/valid dataset\n",
    "        logger.info(\n",
    "            f\"Creating train/valid datasets: {len(train_set):_}/{len(valid_set):_} pts\"\n",
    "        )\n",
    "        self.ds_train = train_set\n",
    "        self.ds_valid = valid_set\n",
    "\n",
    "        # create grid-coordinates\n",
    "        # create dataset from grid coordinates\n",
    "        # predict dataset\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.ds_train,\n",
    "            batch_size=self.dataloader.batchsize_train,\n",
    "            shuffle=self.dataloader.shuffle_train,\n",
    "            num_workers=self.dataloader.num_workers_train,\n",
    "            pin_memory=self.dataloader.pin_memory_train,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.ds_valid,\n",
    "            batch_size=self.dataloader.batchsize_valid,\n",
    "            shuffle=self.dataloader.shuffle_valid,\n",
    "            num_workers=self.dataloader.num_workers_valid,\n",
    "            pin_memory=self.dataloader.pin_memory_valid,\n",
    "        )\n",
    "\n",
    "    def predict_dataloader(self):\n",
    "        raise NotImplementedError()\n",
    "        # return torch.utils.data.DataLoader(\n",
    "        #     self.ds_predict,\n",
    "        #     batch_size=self.dataloader.batchsize_valid,\n",
    "        #     shuffle=self.dataloader.shuffle_valid,\n",
    "        #     num_workers=self.dataloader.num_workers_valid,\n",
    "        #     pin_memory=self.dataloader.pin_memory_valid\n",
    "        # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = get_demo_config()\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = AlongTrackDataModule(\n",
    "    data=config.data,\n",
    "    preprocess=None,\n",
    "    traintest=config.traintest,\n",
    "    dataloader=config.dataloader,\n",
    "    eval=None,\n",
    ")\n",
    "\n",
    "dm.setup()\n",
    "\n",
    "train_ds = dm.train_dataloader()\n",
    "valid_ds = dm.val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('torch_py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ba845ae818c285ca2fe9389acfa2d2da9f6f964e42b65478d402ad448a072775"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
