{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from pyprojroot import here\n",
    "\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "\n",
    "# spyder up to find the root\n",
    "root = here(project_files=[\".root\"])\n",
    "exp = here(\n",
    "    relative_project_path=root.joinpath(\"experiments/dc21a\"), project_files=[\".local\"]\n",
    ")\n",
    "\n",
    "\n",
    "# append to path\n",
    "sys.path.append(str(root))\n",
    "sys.path.append(str(exp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import Compose\n",
    "import xarray as xr\n",
    "from inr4ssh._src.datasets import AlongTrackDataset\n",
    "from inr4ssh._src.datasets.utils import get_num_training\n",
    "\n",
    "from inr4ssh._src.transforms.dataset import (\n",
    "    TimeJulianMinMax,\n",
    "    TimeJulian,\n",
    "    TimeMinMax,\n",
    "    ToTensor,\n",
    ")\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "spatial_columns = [\"lon\", \"lat\"], [\"x\", \"y\", \"z\"], [\"lon_rad\", \"lat_rad\"]\n",
    "temporal_columns = [\"time\"], [\"vtime\"]\n",
    "```\n",
    "\n",
    "**Transformations**\n",
    "\n",
    "* Spherical 2 Cartesian\n",
    "* Spherical Degrees to Radians\n",
    "* Temporal to Julian\n",
    "* TimeStamps 2 Days of the Year\n",
    "* TimeStamps 2 Cycles\n",
    "* Temporal Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_link = \"/Volumes/EMANS_HDD/data/dc20a_osse/test/ml/nadir1.nc\"\n",
    "\n",
    "ds = xr.open_dataset(ds_link)\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Transforms\n",
    "\n",
    "So there are a few transformations we should do within the dataset: 1) timestamps and 2) torch tensors. In general, the xarray datasets will almost always have numpy arrays for the spatial and output values. So we need to change them into torch tensors. We also have datetime64 data structures for the time values. So we need to transform those into numerical values and additionally into torch tensors.\n",
    "\n",
    "**Note**: There are other additional transformations we can do, e.g. spherical, cartesian, etc, but I decided to offload them to the `trainer` (which will be discussed later). In general, the dataset transformations should only have transformations that change numpy arrays to torch tensors \n",
    "\n",
    "There are some available transformations within the library:\n",
    "* Julian Time (Temporal Transform)\n",
    "* TimeMinMax (Temporal Transform)\n",
    "* ToTensor (Spatial, Temporal, Output Transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define column names\n",
    "spatial_columns = [\"lon\", \"lat\"]\n",
    "temporal_columns = [\"time\"]\n",
    "output_columns = [\"ssh_model\"]\n",
    "\n",
    "# initialize dataset\n",
    "torch_ds = AlongTrackDataset(\n",
    "    ds, spatial_columns, temporal_columns, output_columns, transform=None\n",
    ")\n",
    "\n",
    "batchsize = 32\n",
    "\n",
    "ibatch = torch_ds[0:batchsize]\n",
    "\n",
    "ibatch[\"spatial\"].shape, ibatch[\"temporal\"].shape, ibatch[\"output\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define column names\n",
    "spatial_columns = [\"lon\", \"lat\"]\n",
    "temporal_columns = [\"time\"]\n",
    "output_columns = [\"ssh_model\"]\n",
    "\n",
    "# initialize dataset\n",
    "torch_ds = AlongTrackDataset(\n",
    "    ds, spatial_columns, temporal_columns, output_columns=None, transform=None\n",
    ")\n",
    "\n",
    "batchsize = 32\n",
    "\n",
    "ibatch = torch_ds[0:batchsize]\n",
    "\n",
    "ibatch[\"spatial\"].shape, ibatch[\"temporal\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Compose(\n",
    "    [\n",
    "        # TimeMinMax(),\n",
    "        # TimeJulian(),\n",
    "        TimeJulianMinMax(),\n",
    "        ToTensor(),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize dataset\n",
    "torch_ds = AlongTrackDataset(\n",
    "    ds, spatial_columns, temporal_columns, output_columns, transform=transform\n",
    ")\n",
    "\n",
    "batchsize = 32\n",
    "\n",
    "ibatch = torch_ds[0:batchsize]\n",
    "\n",
    "ibatch[\"spatial\"].shape, ibatch[\"temporal\"].shape, ibatch[\"output\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize dataset\n",
    "torch_ds = AlongTrackDataset(\n",
    "    ds, spatial_columns, temporal_columns, output_columns=None, transform=transform\n",
    ")\n",
    "\n",
    "batchsize = 32\n",
    "\n",
    "ibatch = torch_ds[0:batchsize]\n",
    "\n",
    "ibatch[\"spatial\"].shape, ibatch[\"temporal\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ibatch[\"temporal\"].min(), ibatch[\"temporal\"].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility: `pd.DataFrame`, `xr.Dataset`\n",
    "\n",
    "For inference/predictions, we will often have a dataset we want for predictions, and then we want to extract a dataframe and/or dataset with all of the coordinates. These utility functions will help do that using the attributes within the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize dataset\n",
    "torch_ds = AlongTrackDataset(\n",
    "    ds, spatial_columns, temporal_columns, output_columns, transform=transform\n",
    ")\n",
    "\n",
    "outputs = torch_ds[:][\"output\"]\n",
    "\n",
    "ds_ = torch_ds.create_predict_df(outputs)\n",
    "\n",
    "ds_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize dataset\n",
    "torch_ds = AlongTrackDataset(\n",
    "    ds, spatial_columns, temporal_columns, output_columns=None, transform=transform\n",
    ")\n",
    "\n",
    "\n",
    "ds_ = torch_ds.create_predict_df(outputs)\n",
    "\n",
    "ds_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_ = torch_ds.create_predict_ds(outputs)\n",
    "ds_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_prct = 0.9\n",
    "num_train, num_valid = get_num_training(len(torch_ds), train_prct=train_prct)\n",
    "train_split_seed = 42\n",
    "\n",
    "train_set, valid_set = torch.utils.data.random_split(\n",
    "    torch_ds,\n",
    "    [num_train, num_valid],\n",
    "    generator=torch.Generator().manual_seed(train_split_seed),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader\n",
    "\n",
    "So finally, we can easily put this in a dataloader. This makes things really easy in terms for generating batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True)\n",
    "valid_dl = torch.utils.data.DataLoader(valid_set, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ibatch = next(iter(torch_dl))\n",
    "for ibatch in train_dl:\n",
    "    break\n",
    "\n",
    "for ivbatch in valid_dl:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ibatch[\"spatial\"].shape, ibatch[\"temporal\"].shape  # , ibatch[\"output\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ivbatch[\"spatial\"].shape, ivbatch[\"temporal\"].shape  # , ivbatch[\"output\"].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('torch_py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ba845ae818c285ca2fe9389acfa2d2da9f6f964e42b65478d402ad448a072775"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
